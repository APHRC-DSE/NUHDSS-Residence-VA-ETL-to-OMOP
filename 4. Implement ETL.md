### Create database and schema using PSQL shell
````sql
-- Connect to PostgreSQL (replace username with your username)
psql -U username

-- Create a database
CREATE DATABASE database_name;

-- Connect to the new database
\c database_name

-- Create a schema
CREATE SCHEMA schema_name;

-- List databases
\l

-- List schemas
\dn

-- Disconnect from PostgreSQL
\q
````
### Create OMOP Tables using DDL

--This script will demonstrate how the CDM R package can be used to create the CDM tables in your environment. 
--the script shows the executeDdl function that will connect up to your SQL client directly (assuming your dbms is one of the supported dialects) and instantiate the tables through R.

- First, install the package from GitHub
```R
install.packages("devtools")
devtools::install_github("OHDSI/CommonDataModel")
```


- List the currently supported SQL dialects and supported CDM versions
````R
CommonDataModel::listSupportedDialects()
CommonDataModel::listSupportedVersions()
````
- This function will generate the text files in the dialect you choose, putting the output files in the folder you specify.
````R
CommonDataModel::buildRelease(cdmVersions = "5.4",
                              targetDialects = "postgresql",
                              outputfolder = "/Output")  #setwd
````
- If you have an empty schema ready to go, the package will connect and instantiate the tables for you. To start, you need to download DatabaseConnector in order to connect to your database.
````R
install.packages("DatabaseConnector")
library(DatabaseConnector)
````
- Download and set JDBC drivers
````
downloadJdbcDrivers("postgresql")
Sys.setenv("DATABASECONNECTOR_JAR_FOLDER" = "path/to/jdbcDrivers")
````

````R
extraSettings <- ";databaseName=alpha;integratedSecurity=false;encrypt=false;trustServerCertificate=true;sslProtocol=TLSv1"

cd <- DatabaseConnector::createConnectionDetails(dbms = "postgresql",
                                                 server = "localhost/databasename",  
                                                 user = "postgres",
                                                 password = "yourpassword",
                                                 pathToDriver = "c:/temp/jdbcDrivers",
                                                 extraSettings = extraSettings
)
conn <- connect(cd) #test connection
CommonDataModel::executeDdl(connectionDetails = cd,
                            cdmVersion = "5.4",
                            cdmDatabaseSchema = "public"
)
````
## Implement SQL scripts 
For this project, I used PostrgreSQL tom implement ETL process. 
### 1. Alter tables to drop constraints 
````SQL
BEGIN;
ALTER TABLE public.concept DROP CONSTRAINT fpk_concept_domain_id;
ALTER TABLE public.concept DROP CONSTRAINT fpk_concept_vocabulary_id;
ALTER TABLE public.concept DROP CONSTRAINT fpk_concept_concept_class_id;
ALTER TABLE public.concept_relationship DROP CONSTRAINT fpk_concept_relationship_relationship_id;
ALTER TABLE public.concept_relationship DROP CONSTRAINT fpk_concept_relationship_concept_id_2;
ALTER TABLE public.concept_relationship DROP CONSTRAINT fpk_concept_relationship_concept_id_1;
ALTER TABLE public.CONCEPT_ANCESTOR DROP CONSTRAINT fpk_concept_ancestor_ancestor_concept_id;
ALTER TABLE public.CONCEPT_ANCESTOR DROP CONSTRAINT fpk_concept_ancestor_descendant_concept_id;
ALTER TABLE public.CONCEPT_SYNONYM DROP CONSTRAINT fpk_concept_synonym_concept_id;
END;
````
### 2. Load latest OMOP vocabulary to the CDM schema
Latest version of OMOP vocabularies can be downloaded from athena and copied to CDM schema. Alternatively, you can restore Vocabulary schema in your DB. 
````sql
BEGIN;
COPY nuhdss_cdm.CONCEPT FROM 'path/to\CONCEPT.csv'
WITH DELIMITER E'\t' 
CSV HEADER QUOTE E'\b';

COPY nuhdss_cdm.CONCEPT_RELATIONSHIP FROM 'path\to\CONCEPT_RELATIONSHIP.csv' 
WITH DELIMITER E'\t' 
CSV HEADER QUOTE E'\b';

COPY nuhdss_cdm.CONCEPT_ANCESTOR FROM 'path\to\CONCEPT_ANCESTOR.csv'
WITH DELIMITER E'\t' 
CSV HEADER QUOTE E'\b';

COPY nuhdss_cdm.CONCEPT_SYNONYM FROM 'path\to\CONCEPT_SYNONYM.csv'
WITH DELIMITER E'\t' 
CSV HEADER QUOTE E'\b' ;

COPY nuhdss_cdm.DRUG_STRENGTH FROM 'path\to\DRUG_STRENGTH.csv' 
WITH DELIMITER E'\t' 
CSV HEADER QUOTE E'\b';

COPY nuhdss_cdm.VOCABULARY FROM 'path\to\VOCABULARY.csv'
WITH DELIMITER E'\t' 
CSV HEADER QUOTE E'\b' ;

COPY nuhdss_cdm.RELATIONSHIP FROM 'path\to\RELATIONSHIP.csv'
WITH DELIMITER E'\t' 
CSV HEADER QUOTE E'\b' ;

COPY nuhdss_cdm.CONCEPT_CLASS FROM 'path\to\CONCEPT_CLASS.csv'
WITH DELIMITER E'\t' 
CSV HEADER QUOTE E'\b' ;

COPY nuhdss_cdm.DOMAIN FROM 'path\to\DOMAIN.csv'
WITH DELIMITER E'\t' 
CSV HEADER QUOTE E'\b' ;
END;
````

### 3. Create staging table and copy source data
The staging table will host source data that will be transformed to CDM tables. 
- NUHDSS Staging table structure
````sql
CREATE TABLE nuhdss_staging_tables.nuhdss_data(
	ID VARCHAR,
	slum_area_in_NUHDSS	VARCHAR,
	round_in_which_event_occurred VARCHAR,
	residency_event_date VARCHAR,
	residency_event	VARCHAR,
	date_of_birth_of_NUHDSS_individual	VARCHAR,
	gender_of_NUHDSS_individual	VARCHAR,
	ethnicity_of_NUHDSS_individual	VARCHAR,
	current_religion VARCHAR,
	highest_level_of_school_ever_attended	VARCHAR,
	type_of_area_in_Kenya_from_which_individual_came VARCHAR,
	First_cause_of_death_by_InterVA		VARCHAR,
	broad_first_cause_of_death_by_InterVA	VARCHAR,
	general_first_cause_of_death_by_InterVA	VARCHAR,
	general_immediate_cause_of_death_final	VARCHAR,
	broad_underlying_cause_of_death_final	VARCHAR,
	general_underlying_cause_of_death_final	VARCHAR,
	broad_immediate_cause_of_death_final	VARCHAR,
	Event	VARCHAR,
	conceptId	INT,
	conceptName	VARCHAR,
	Cleaned_COD	VARCHAR
	);
````
- Add source data

````sql
  COPY nuhdss_staging_tables.nuhdss_data
	FROM 'path\to\data.csv'
	DELIMITER ','
	CSV HEADER
	NULL 'NA';
````
